{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angus-Eastell/Intro_to_AI/blob/main/7_2_nns_for_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural networks for MNIST\n",
        "\n",
        "Now, we're going to put everything together and train a fully connected network to recognise MNIST handwritten digits.\n",
        "\n",
        "The point of this notebook is for you to have a play with tweaking the optimizer.\n",
        "\n",
        "Here are some things to try:\n",
        "* changing the SGD learning rate.\n",
        "* changing the SGD momentum.\n",
        "* changing the optimizer to Adam.\n",
        "* changing the Adam learning rate.\n",
        "* changing the `beta1` and `beta2` parameters in Adam.\n",
        "\n",
        "Overall, what's the best result you can get after 5 epochs?  Is your best result with Adam or SGD?\n",
        "\n",
        "You will need to check out the docs for [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) and [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
        "\n",
        "The notebook will be faster on GPU, but its still perfectly fine on CPU.  Remember, to switch to GPU, go to \"Runtime\" -> \"Change Runtime Type\"."
      ],
      "metadata": {
        "id": "Ujbbo2lOAiUy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-CQK29k_EzX",
        "outputId": "22fb9927-1918-499a-8b82-c3c023c62897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy after 1 epochs: 95.39 %\n",
            "Test accuracy after 2 epochs: 96.78 %\n",
            "Test accuracy after 3 epochs: 97.32 %\n",
            "Test accuracy after 4 epochs: 97.55 %\n",
            "Test accuracy after 5 epochs: 97.67 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Check whether we have a GPU.  Use it if we do.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "# MNIST train and test datasets.  I'm not going to talk about these in this course.\n",
        "# you should just be able to follow \"recipes\" online.\n",
        "train_dataset = torchvision.datasets.MNIST(root='data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# MNIST train and test datasets.  I'm not going to talk about these in this course.\n",
        "# However, note that I'm using a much bigger batch size at test-time.  That's\n",
        "# because at training time, we have to backprop, so we have to save all the\n",
        "# intermediate variables, which takes alot of memory.  We don't have to do that\n",
        "# at test-time.\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=1000,\n",
        "                                          shuffle=False)\n",
        "# Define network\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, num_classes)\n",
        ").to(device)\n",
        "\n",
        "#############################\n",
        "#### Tweak this line !!! ####\n",
        "#############################\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.015, momentum= 0.95)\n",
        "\n",
        "def train():\n",
        "    # Does one training epoch (i.e. one pass over the data.)\n",
        "    for images, labels in train_loader:\n",
        "        # Move tensors to the GPU device, and convert image to vector.\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(images)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss = nn.functional.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "def test(epoch):\n",
        "    # Do one pass over the test data.\n",
        "    # In the test phase, don't need to compute gradients (for memory efficiency)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            #Convert image pixels to vector\n",
        "            images = images.reshape(-1, 28*28).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(images)\n",
        "\n",
        "            # Compute total correct so far\n",
        "            predicted = torch.argmax(logits, -1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        print(f'Test accuracy after {epoch+1} epochs: {100 * correct / total} %')\n",
        "\n",
        "\n",
        "# Run training\n",
        "for epoch in range(5):\n",
        "    train()\n",
        "    test(epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SDG:\n",
        "Impact of altering the learning rate for SDG:\n",
        "- Lowering learning rate reduces accuracy in five epochs as there is not enough time to converge to the optimal gradient descent or could get stuck in poor local minima.\n",
        "- Increasing learning rate increases the speed in which the neural network can get to high accuracies, however it can cause oscillations and instablility around the optimal loss.\n",
        "- Ideal around 0.1-0.3\n",
        "Best accuracy was 97.87% at lr = 0.2\n",
        "\n",
        "Impact of adding momentum-\n",
        "- Low momentum increases time to conversion as it relies more on the current gradient rather than past gradients, this slows convergence. It can also lead to less stability with higher learning rates.\n",
        "- High momentum could lead to overshooting of optimal gradients wiht reduced responsivness to new gradients. It can also lead to higher instability with high learing rates.\n",
        "- Ideal momentum around 0.9\n",
        "Best accuracy was 97.67% at lr = 0.015 and momentum 0.95"
      ],
      "metadata": {
        "id": "-GN2jDgLx68u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adam:\n",
        "- Learnign rate of adam needs to be significantly lower around 0.001 as higher learning rates even at 0.01 can lead to instability and as adam needs less learning time, due to using both momentum and adaptive learning, a high learning rate is unnecessary.\n",
        "\n",
        "Adjusting betas:\n",
        "- Beta 1 should be around 0.9 or range (0.85 - 0.95) and dictates the exponential moving average beta term. If beta 1 is too low (around 0.5) it can make the optimisation to repsonsive to recent updates making it more unstable. If beta 1 is too high around (0.99) it can make it less responsive to recent gradients and slow to react to change in loss in the landscape.\n",
        "- Beta 2 should be around 0.999 and represents the RMS moving average term. If beta 2 is too low (around 0.9) the optimisation will react to changes in the landscape faster causing step sizes to fluctuate too much. If beta 2 is too high (around 0.9999) it increases the stability of the optimisation however it can lead to slow adaptation to steep gradients.\n",
        "\n",
        "Best found at  lr=0.001, betas = (0.9, 0.995) at 98%\n"
      ],
      "metadata": {
        "id": "JauvVyPH1Ixe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall adam outperformed SDG with and without momentum over 5 epochs."
      ],
      "metadata": {
        "id": "U_o-8eI2HLET"
      }
    }
  ]
}
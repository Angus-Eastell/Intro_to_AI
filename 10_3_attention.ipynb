{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angus-Eastell/Intro_to_AI/blob/main/10_3_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Attention\n",
        "\n",
        "Consider some inputs and attention weights"
      ],
      "metadata": {
        "id": "XCU6w2qCeHst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qmnbIGOGd-ix"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "t.manual_seed(0)\n",
        "\n",
        "H = 10            #Embedding dimension (i.e. dimension of inputs)\n",
        "S = 4             #Sequence length\n",
        "D = 3             #Dimension of keys/values\n",
        "Dv = 5            #Dimension of queries\n",
        "\n",
        "X = t.randn(S, H) #Example inputs\n",
        "Wq = t.randn(H, D)\n",
        "Wk = t.randn(H, D)\n",
        "Wv = t.randn(H, Dv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the queries, keys and values"
      ],
      "metadata": {
        "id": "CkaLxjg7BmYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here\n",
        "\n",
        "Q = X @ Wq\n",
        "K = X @ Wk\n",
        "V = X @ Wv"
      ],
      "metadata": {
        "id": "WFun6K6EECqi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "Q = X @ Wq\n",
        "K = X @ Wk\n",
        "V = X @ Wv"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FqC7rl1VBzDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that computes the attention matrix,\n",
        "\\begin{align}\n",
        "  A_{ij}(\\mathbf{Q}, \\mathbf{K}) = \\frac{\\exp(\\mathbf{Q}_{i, :} \\cdot \\mathbf{K}_{j, :})}{\\sum_k \\exp(\\mathbf{Q}_{i, :} \\cdot \\mathbf{K}_{k, :})}\n",
        "\\end{align}\n",
        "Check that the results of your function have the right shape, and sum to $1$ in the right way."
      ],
      "metadata": {
        "id": "iUmcEocOeouV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here\n",
        "\n",
        "# @ computes dot product\n",
        "# summing over last dimensions which corresponds to k\n",
        "# need to keep original dimensions to ensure tensor keeps the same shape\n",
        "A = t.exp(Q @ K.mT)/ t.sum(t.exp(Q @ K.mT), dim = -1, keepdim= True)\n",
        "\n",
        "# From solution\n",
        "#Results should be 4 x 4:\n",
        "assert A.shape == (S, S)\n",
        "#And columns should sum to 1:\n",
        "print(A.sum(-1))\n",
        "print(A)"
      ],
      "metadata": {
        "id": "eKoBNWhFEBwz",
        "outputId": "f734871f-c4e5-4d67-8b32-3b1efed44a12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([[2.4771e-14, 2.7799e-12, 1.0000e+00, 2.0112e-15],\n",
            "        [7.8475e-16, 4.0728e-13, 1.0000e+00, 1.2259e-10],\n",
            "        [3.9596e-03, 3.9879e-03, 1.3989e-04, 9.9191e-01],\n",
            "        [5.4816e-09, 1.9935e-12, 8.3131e-18, 1.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "def compute_A(K, Q):\n",
        "    exp_QK = (Q@K.mT).exp()\n",
        "    return exp_QK / exp_QK.sum(-1, keepdim=True)\n",
        "\n",
        "A = compute_A(K, Q)\n",
        "\n",
        "#Results should be 4 x 4:\n",
        "assert A.shape == (S, S)\n",
        "#And columns should sum to 1:\n",
        "print(A.sum(-1))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NVgKuuMTeZc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "348e82c2-975c-4f92-974d-56885bea1d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that computes the causal attention matrix,\n",
        "\\begin{align}\n",
        "  A_{ij}(\\mathbf{Q}, \\mathbf{K}) = \\frac{\\mathbb{I}(j \\leq i) \\exp(\\mathbf{Q}_{i, :} \\cdot \\mathbf{K}_{j, :})}{\\sum_k \\mathbb{I}(k \\leq i) \\exp(\\mathbf{Q}_{i, :} \\cdot \\mathbf{K}_{k, :})}\n",
        "\\end{align}\n",
        "Check that the results of your function have the right shape, and sum to $1$ in the right way."
      ],
      "metadata": {
        "id": "SXeGeYUrCeXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here\n",
        "shape_mask =(t.exp(Q @ K.mT)).shape\n",
        "# shape of mask needs to be 4 by 4\n",
        "mask = t.zeros(shape_mask)\n",
        "i, j = t.arange(0,mask.shape[0]), t.arange(0,mask.shape[1])\n",
        "for index1,val1 in enumerate(i):\n",
        "  for index2,val2 in enumerate(j):\n",
        "    if val2 <= val1:\n",
        "      mask[index1, index2] = 1\n",
        "\n",
        "causal_A = (mask * t.exp(Q @ K.mT))/ t.sum((mask * t.exp(Q @ K.mT)), dim = -1, keepdim= True)\n",
        "\n",
        "# From solution\n",
        "#Results should be 4 x 4:\n",
        "assert causal_A.shape == (S, S)\n",
        "#And columns should sum to 1:\n",
        "print(causal_A.sum(-1))\n",
        "\n",
        "print(causal_A)\n"
      ],
      "metadata": {
        "id": "5dyP5nulEDby",
        "outputId": "c65ba3c5-cb7f-4e7a-9139-627d33fc18bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n",
            "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.9231e-03, 9.9808e-01, 0.0000e+00, 0.0000e+00],\n",
            "        [4.8960e-01, 4.9310e-01, 1.7297e-02, 0.0000e+00],\n",
            "        [5.4816e-09, 1.9935e-12, 8.3131e-18, 1.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "def compute_causal_A(K, Q):\n",
        "    range = t.arange(S)\n",
        "    i = range[:, None]\n",
        "    j = range[None, :]\n",
        "    mask = j <= i\n",
        "\n",
        "    exp_QK = (Q@K.mT).exp()\n",
        "    masked_exp_QK = mask * exp_QK\n",
        "    return masked_exp_QK / masked_exp_QK.sum(-1, keepdim=True)\n",
        "\n",
        "causal_A = compute_causal_A(K, Q)\n",
        "#Results should be 4 x 4:\n",
        "assert causal_A.shape == (S, S)\n",
        "#And columns should sum to 1:\n",
        "print(causal_A.sum(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXJXyJG9Cp9i",
        "outputId": "42a3bf87-be51-41e8-8166-c8cab53421c1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that computes the output of causal self-attention:\n",
        "\\begin{align}\n",
        "\\text{self-attention}_{i, :}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= \\sum_j A_{ij}(\\mathbf{Q}, \\mathbf{K}) V_{j, :}\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "_WNfZx_nf4lR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Your code here\n",
        "# use mat mul to find weighted sum\n",
        "attension = causal_A @ V\n",
        "print(attension.shape)\n",
        "attension"
      ],
      "metadata": {
        "id": "NIWM0-xVEEgG",
        "outputId": "a5d87bed-8ae9-4036-c772-c37374a9f740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7919, -2.3897,  3.8101,  2.2223, -0.2126],\n",
              "        [-1.0591,  1.0445,  3.9767,  1.7151,  1.5959],\n",
              "        [-0.8514, -0.6575,  3.8082,  1.9692,  0.6545],\n",
              "        [ 0.3252,  4.1818, -2.1640,  0.4850,  4.6732]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer\n",
        "def compute_self_attention(Q, K, V):\n",
        "    return compute_causal_A(Q, K) @ V\n",
        "self_attention = compute_self_attention(Q, K, V)\n",
        "print(self_attention)\n",
        "assert self_attention.shape == (S, Dv)"
      ],
      "metadata": {
        "id": "U0leuuhkf396",
        "outputId": "1403ebb6-aad7-45b0-998c-0f0dff56e229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7919, -2.3897,  3.8101,  2.2223, -0.2126],\n",
            "        [-0.7953, -2.3463,  3.8122,  2.2159, -0.1897],\n",
            "        [-0.8133, -2.1141,  3.8235,  2.1816, -0.0674],\n",
            "        [ 0.3252,  4.1817, -2.1640,  0.4850,  4.6732]])\n"
          ]
        }
      ]
    }
  ]
}